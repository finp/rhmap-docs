
[[sync-protocol]]
= Sync Protocol

NOTE: This section uses the FH Sync JS SDK implementation as a reference.

image:sync_flow_chart.png[sync_flow_chart]

1. CRUDL operations on the dataset are invoked using client APIs. 
.. A dataset is a map. The key is the universal unique id (UID) of the data record and the value is an instance of data record.
.. A data record is a map too. It contains 2 keys: data and hash. The data is the actual user data. The hash is the hash value generated from the data. The same user data results in the same hash value.
.. All the SDKs persist client side dataset as files.

2. All write operations result in a pending change. Typically there are a set of pending changes (ChangeSet) on the client.
.. The ChangeSet is a map. The keys are the hash values of the instances of Pending Change.
.. Each Pending Change is similar to the following:
+
[source,json]
----
{
 "uid": <the unique id of the data record. For creation, it's the hash of the pending record>,
 "hash": <the hash value of the pending record>,
 "action": <operation type, should be one of create, update or delete>,
 "post": <the updated user data>,
 "postHash": <the hash value of the updated user data>
 "pre": <the user data before the change>,
 "preHash": <the hash value of the user data before the change>
 "timestamp": <when this pending change is generated>
}
----

3. The sync loop runs at an interval set by the Client App, that is, the value of sync frequency in the client sync configuration.
If the `auto_sync_local_updates` configuration flag is set to true, a sync occurs whenever there are updates. 
During each sync loop, the client SDK first tries to submit all the pending changes to the cloud that have not been previously submitted.
+
Sample request body:
+
[source,json]
----
{
  "fn": "sync",  //the name of the function to invoke in the cloud
  "dataset_id": "myShoppingList",  //the id of the dataset it tries to sync
  "query_params": {},  //the query params set by the client app. Query params can be used to filter the data set returned - e.g. for a specific user, or data within a geo-fenced area. The server side sync handlers need to understand how to use query params to filter data sets returned from the back end.
  "config": {  //the client sync configurations
    "sync_frequency": 15,
    "auto_sync_local_updates": true,
    "notify_client_storage_failed": true,
    "notify_sync_started": true,
    "notify_sync_complete": true,
    "notify_offline_update": true,
    "notify_collision_detected": true,
    "notify_remote_update_failed": true,
    "notify_local_update_applied": true,
    "notify_remote_update_applied": true,
    "notify_delta_received": true,
    "notify_record_delta_received": true,
    "notify_sync_failed": true,
    "do_console_log": true,
    "crashed_count_wait": 1,
    "resend_crashed_updates": true,
    "sync_active": true,
    "storage_strategy": "dom",
    "file_system_quota": 61644800,
    "has_custom_sync": false,
    "icloud_backup": false
  },
  "meta_data": {},  //the custom meta data set by the client app. This meta data will be sent to the cloud data handler to allow developers to limit the scope of the data set
  "dataset_hash": "a6f7dccd2c70c2743c348f46d905e119f10f2298",  //the hash value of the client dataset
  "acknowledgements": [],  //to confirm acknowledged cloud replies from previous calls
  "pending": [{   //the pending changes that haven't been submitted before
    "inFlight": true,
    "action": "update",
    "post": {
      "name": "g6",
      "created": 1444763286767
    },
    "postHash": "d05be16096158efd93e77584569576a52a0ff022",
    "timestamp": 1444772422701,
    "uid": "561d56bb176f7e000000000f",
    "pre": {
      "name": "g5",
      "created": 1444763286767
    },
    "preHash": "c7a80aebb917865b655bb3eee1289735e9809379",
    "hash": "5a53abb526abd460a552135571f62b732ed34728",
    "inFlightDate": 1444772424027
  }],
  "__fh": {  //extra device/app info added by the FH JS SDK
    "cuid": "6C1F4BAFC6EE4D768202B65A7FDEF7DB",
    "cuidMap": null,
    "destination": "web",
    "sdk_version": "FH_JS_SDK/2.10.0",
    "appid": "kceR5H95sLXXqr7ejwYh9zXP",
    "appkey": "8c1b3fba33a7d741e7726dea52efc14e994f7c0c",
    "projectid": "kceR5HZqoDgFRZ7vMJXgPoJ2",
    "connectiontag": "0.0.1"
  }
}
----

4. When the server receives the request, it invokes the function specified in the request body.
.. For the `sync` function, all the pending changes are added to the pending queue, and the server responds to the client.
When processing an item from the pending queue, the server decides if each change can be applied:
For update and delete, there is a collision detection test. The preHash value in the pending change must match the current hash value of the data with the same uid. If the values do not match, the update/delete is not applied, and a collision is reported. It is the developer's responsibility to decide how to handle collisions, for example, by using the server side collisionHandler function.
.. The result of processing each pending change is recorded in the MongoDB of the app.

5. If the pending change can be applied, the sync server calls the corresponding data handler to apply that change to the Dataset Backend.

6. Clients are notified of the processed changes as part of the next sync.
.. If a sync call with pending updates fails, for example, the server returns a 500 error or times out, the client marks those updates as 'crashed'.
The client tries to resend the updates in the next sync loop.
.. When the client receives the results of any pending changes, it iterates through each of the updates and for each update:
... Acknowledges receiving the results of the pending change
... Removes the processed pending change from local change set
... Processes previously pending changes that are failed ("crashed")
... Checks if any of the delayed pending changes can be submitted
... Makes sure the client dataset contains the correct information
+
The response is similar to the following:
+
[source,json]
----
{
  "hash": "61468b040690d5a8ef14568095b887190c80436a",  //the hash value of the same dataset in the cloud. The client will compare it against the client dataset hash to determine if the syncRecords request is required
  "updates": {  //the results of all the pending changes that have been processed by the cloud app
    "hashes": {  //a map of hashes of all the processed pending changes and the results
      "5a53abb526abd460a552135571f62b732ed34728": {
        "cuid": "6C1F4BAFC6EE4D768202B65A7FDEF7DB",
        "type": "applied",
        "action": "update",
        "hash": "5a53abb526abd460a552135571f62b732ed34728",
        "uid": "561d56bb176f7e000000000f",
        "msg": "''"
      }
    },
    "applied": { //the pending changes that have been applied. Similarly, there will be other keys called "failed" and "collisions" for those are not applied               
      "5a53abb526abd460a552135571f62b732ed34728": {
        "cuid": "6C1F4BAFC6EE4D768202B65A7FDEF7DB",
        "type": "applied",
        "action": "update",
        "hash": "5a53abb526abd460a552135571f62b732ed34728",
        "uid": "561d56bb176f7e000000000f",
        "msg": "''"
      }
    }
  }
}
----

7. The client also compares the current dataset's hash value and the hash value of the cloud dataset returned in the previous step. If the hash values do not match, the client invokes another `syncRecords` request, that is, the client sends all the data UIDs in the dataset and their corresponding data hashes. For example:
+
[source,json]
----
{
  "fn": "syncRecords",  //the cloud function name
  "dataset_id": "myShoppingList", 
  "query_params": {}, 
  "clientRecs": {  //the client data records' UIDs and hashes
    "561d002893ef7d0000000017": "8899c109e001e5dc55544f1390c89510db01c9b2",
    "561d00b6ea74200000000001": "983b6438d40229920b8f527510c3c46e581391dc",
    "561d019fea74200000000007": "e63fb354a6f132b4ba791219ea9f83af0cd6b9e4",
    "561d3036176f7e0000000004": "3a4bb885163f73515d36789ad8025a55f50f6f8f",
    "561d3074176f7e0000000006": "7e32fbbe0a4d144e124362d46c9e7d02e595c22d",
    "561d56bb176f7e000000000f": "d05be16096158efd93e77584569576a52a0ff022"
  },
  "__fh": {
    "cuid": "6C1F4BAFC6EE4D768202B65A7FDEF7DB",
    "cuidMap": null,
    "destination": "web",
    "sdk_version": "FH_JS_SDK/2.10.0",
    "appid": "kceR5H95sLXXqr7ejwYh9zXP",
    "appkey": "8c1b3fba33a7d741e7726dea52efc14e994f7c0c",
    "projectid": "kceR5HZqoDgFRZ7vMJXgPoJ2",
    "connectiontag": "0.0.1"
  }
}
----

8. When the cloud receives the request, it compares the client records with the current records in the cloud, and returns the deltas.
.. The Cloud App keeps a copy of the dataset for the client in MongoDB, and periodically synchronizes with the Dataset Backend. The dataset is marked as inactive if there is no activity from the client for a period of time.
+
Sample response:
+
[source,json]
----
{
  "create": {  //the data that is in cloud but not in the client
    "561d8e63fd12f11b1e000005": {
      "data": {
        "name": "h",
        "created": 1444777543903
      },
      "hash": "deed09ce48982efed9bd21c94c7f056f2959cf81"
    }
  },
  "update": { //the data that does not match
    "561d56bb176f7e000000000f": {
      "data": {
        "name": "g7",
        "created": 1444763286767
      },
      "hash": "63248b16951fbaa50b1513e9d722f0d12a113403"
    }
  },
  "delete": {  //the data that is in the client but not in the cloud
  },
  "hash": "72489ccd1b64ad08a08cb5ed6706228668e6a345" //the global dataset hash
}
----
9. When the client receives the response, it merges the pending changes with the delta, and updates the local dataset. This merge is required because the user can change data in the time after the first request is finished and before the second request is finished. Those changes are not submitted to the cloud at this point.

.. If there are any pending changes, remove the corresponding delta from the response as it is not up to date.
.. Apply the rest of the delta to the dataset.
.. For failed or collided pending changes, as described in step 6b, once the client acknowledges that those changes have been processed by the cloud, it removes those pending changes from the client side change set. At this point, one of the following is true:  
... There are no subsequent pending changes based on these failed/collided changes. In this case, the cloud response is applied to the current dataset for those records immediately and users see those records are updated to the value in the cloud.
... There are subsequent pending changes based on these failed/collided changes (delayed pending changes).  In this case, since those pending changes are still in the client change set, the local value is kept and those changes are submitted during the next sync loop. However, it is likely those changes will fail or cause collisions too. Then the scenario above applies and the client data is also reverted. 

At this point, one sync loop is completed and the same steps apply for the next loop.

The first request is responsible for sending a patch from the client to the cloud, and the second request download a patch from the cloud to the client. For example, given the dataset A, and its initial state A1 on both client side and cloud side:

* Initial state:
** client = A1, cloud = A1
* User making changes on the client: 
** client = B1 = A1 + diff(A1, B1)
* The first request submits diff(A1, B1) to the cloud: 
** cloud = A1 + diff(A1, B1) = B1
* In the meantime, cloud has other changes from other clients: 
** cloud = B1 + diff(B1, C1) = C1
* In the meantime, the user has made more changes on the client:
** client = D1 = B1 + diff(B1, D1)
* The second request sends the current client status D1 to the cloud, and the cloud currently has the status C1, so the client receives diff(D1, C1).  Applying the response to the client, it becomes:
** client = D1 + diff(D1, C1) + diff(B1, D1) = C1 + diff(B1, D1)
** cloud = C1

At this stage, the client has got the cloud data, and its own new data. The new changeset is submitted during next sync loop. When the situation becomes diff(B1,D1) == null, then C1 = C1 and the client and cloud are synchronized.

== Squash Pending Changes

In order to save space, the Sync Framework uses a technique called "squashing". If more than one change is made to a record before a sync loop occurs, only the value before all those changes and the very last change is saved. All the intermediate changes are discarded.

For example, given the record current value is A. The user makes a few changes to the record to change it from A to B, then B to C, then C to D. At the end, in the sync request, the pending change only contains:

----
pre: A
post: D
----

The method to achieve this is to use another internal map (called meta, this is different from the meta data that can be set using the API) to track if there are existing pending changes for a given UID. For example, given a record with UID uid1, its value changes from A to B, there will be a new pending change in the changeset (call it P1), and the hash value of this pending change is hash(AB). This is saved in the meta:

[source,json]
----
{
  "uid1": {
    "fromPending": true,
    "pendingUid": hash(AB)
  }
}
----

Then the value changes from B to C, which results in another pending change (P2) with hash value hash(BC). The sync client looks up the meta and it sees there is already a pending change for this data record and it is not submitted. Then it uses the "pendingUid" value (the hash of the previous pending change) to locate the pending change, and update the post value of P1 to the post value of P2:

* P1.post = P2.post = C;
* P1.postHash = P2.postHash = hash(BC) ;

After this, P2 is discarded.

a different strategy is used for other operations:

* If the current pending change operation is "create" and there is a previous pending change.
** This should be a rare case, for example, double submission from the client.  The previous pending change is deleted
* If the current pending change operation is "delete"
** If the previous change is "create", they cancel each other. Both changes are removed from the change set.
** If the previous change is "update", the current pending change's pre value changes to the previous change's pre value. For example, A changes to B (P1) and then deleted (P2). In this case, the change of A to B is removed:
*** P2.pre = P1.pre = A
*** P2.preHash = P1.preHash = hash(A)
*** delete P1

One thing to notice is that squashing does not happen if the previous pending change has been submitted, that is, the *inflight* flag of the previous pending change is set to true. 

== Crashed Pending Changes

As mentioned earlier, the first sync request could fail due to network errors, time outs, etc. In this case, the pending changes submitted in those requests are marked as "crashed".

The re-submission of the crashed pending changes can be controlled using 2 configuration options:

* `crash_count_wait`: how many sync loops before re-submitting the crashed changes.
* `resend_crashed_updates`: should the crashed updates be submitted again

You can use the submission delay to avoid causing cascading failures on the server. In most cases, it is not an issue and you should consider always re-submitting crashed pending changes, by setting `crash_count_wait` to 0.

== Delayed Pending Changes

This example explains how the delayed flag can be used:

Given a record with UID uid1, and it's current value A. The user first changes the value from A to B, which results in a pending change called P1 (hash value: hash(AB)).

Then P1 is submitted. At the same time, the user changes the value from B to C, resulting in pending change P2 (hash value: hash(BC)). Because P1 is being submitted, P2 is not squashed into P1.

If the P1 submission fails and is marked as crashed, and the app is configured to re-submit the crashed pending changes immediately in next sync loop, there are 2 pending changes in the change set:

[source,json]
----
{
 hash(AB): {
   uid: uid1,
   pre: A,
   post: B
  },
  hash(BC): {
   uid: uid1,
   pre: B,
   post: C
  }
}
----

When the next sync loop starts, the change set is converted into an array of pending changes during the request. However, since the change set is a map, you can not be sure of the order of the pending changes in the array, it could end up with [P1, P2] in the pending array, or [P2, P1] in the pending array.

If it is the former, the changes are applied. If it is the latter, it results in a conflict, and none of the changes are applied.

To fix this issue, the "delayed" flag is introduced to the pending changes. It means the pending change should not be submitted as there are previous changes that are being submitted and have not got a response from the cloud yet. 

In this case, because P1 is being submitted, then P2 is marked as delayed and it is not submitted until P1 is resolved. The P1 can be resolved using the response of the first sync request. An extra step is required to check if any of the delayed pending changes can be sent in the next sync loop.

== Hash Algorithm

In order to generate the same hash across different client SDKs and the cloud SDK, a simple algorithm is used to make sure the data is always serialized into the same format. It is shown in the following pseudocode:

[source,json]
----
var out = [];
function sortObj(data){
    var keys = data.keys();
    keys = sort(keys);  //should use the unicode code points, see javascript's sort for reference
    for key in keys:
        var value = data[key];
        if typeof value == "string":
            out.push({"key": key, "value": data[key]})
         else:
           sortObj(data[key]);
}
----


For example, data {a:1, b:2, c:3} are converted to:

[source,json]
----
[{"key":"a", "value": 1}, {"key":"b", "value": 2}, {"key":"c", "value": 3}]
----

Then SHA1 hash is used to generate the hash value.

NOTE: The client side hashing will be deprecated in the future. After that, the hash value of data records and datasets will always be regenerated on the cloud side.

== UID Changes for Data Created on the Client

As described in the sync protocol, when a new data record is created on the client, a temporary UID is generated on the client and assigned to that record. Once the data is synced to the cloud, the the permanent UID is returned in the response of the first sync request.

Sample request body, only the pending part is listed here:

[source,json]
----
{
  ...
  "pending": [{
    "inFlight": true,
    "action": "create",
    "post": {
      "name": "i",
      "created": 1444826652192
    },
    "postHash": "8619f71cf44f2fbf90d40ca9f8769d603fb42aae",
    "timestamp": 1444826652193,
    "hash": "6b4419dd66d0ff72f3bdb5796617af64c8e0d89b",
    "uid": "6b4419dd66d0ff72f3bdb5796617af64c8e0d89b",  //temporary UID, it is the hash value of this pending record
    "inFlightDate": 1444826663091
  }],
  ...
}
----
Sample Response:

[source,json]
----
{
  "hash": "fdbaab8279ba8d6035ccc6eb32783513e02a1c93",
  "updates": {
    "hashes": {
      "6b4419dd66d0ff72f3bdb5796617af64c8e0d89b": {
        "cuid": "6C1F4BAFC6EE4D768202B65A7FDEF7DB",
        "type": "applied",
        "action": "create",
        "hash": "6b4419dd66d0ff72f3bdb5796617af64c8e0d89b",
        "uid": "561e4e45fd12f11b1e000008",
        "msg": "''"
      }
    },
    "applied": {
      "6b4419dd66d0ff72f3bdb5796617af64c8e0d89b": {
        "cuid": "6C1F4BAFC6EE4D768202B65A7FDEF7DB",
        "type": "applied",
        "action": "create",
        "hash": "6b4419dd66d0ff72f3bdb5796617af64c8e0d89b",  //the temporary UID from the client
        "uid": "561e4e45fd12f11b1e000008", //the real UID in the cloud
        "msg": "''"
      }
    }
  }
}
----

However, this change of UID could cause problems, for example,  the app could save the data record into its own database, and later on if the app tries to read the same data again using the UID from local saved data, it might not be able to find the data record because the UID has changed.

To solve this problem, the Sync framework uses a map to track the UID changes.

For example, every time the response of the first sync request is received, the client SDK iterates through the applied pending changes and looks for any "create" replies. If there are any, it adds the hash value (old UID) and the new uid to the new UID tracking map.

Then when read/update/delete API is called, it always checks if the UID passed in is in the UID tracking map. If it is, it gets the real UID and uses that instead.

== Events

Various events are emitted at different stages of the sync loop:

image:sync_events.png[Synchronization Events]

Some of the client SDKs emit those events by default, for example the JS SDK, and some SDKs do not emit those events by default, for example, the iOS and Android SDK. This default can be changed in the `SyncConfig` object. The .NET SDK only emits those events if there are corresponding listeners set. This default may be changed in future releases of {ProductShortName}.

When the events are enabled, each of the listeners is invoked with a notification parameter. This notification parameter contains different fields for different events:

=== local_update_applied

An event of this type is emitted after a record change is saved locally:

* datasetId: the id of the dataset
* uid: the uid of the saved data record
* code: the type of the event (local_update_applied)
* message: the name of the operation (e.g. create, update)

=== sync_started

An event of this type is emitted after the sync loop starts:

* datasetId: the id of the dataset
* uid: null
* code: the type of the event (sync_started)
* message: null

=== remote_update_applied/remote_update_failed/collision_detected

One event emitted for each of the processed pending changes returned from the Cloud App:

* applied -> remote_update_applied, failed -> remote_update_failed, collision -> collision_detected
* datasetId: the id of the dataset
* uid: the uid of the record
* code: the type of the event
* message: the json object return in the "updates" response. For example:
+
----
"cuid": "6C1F4BAFC6EE4D768202B65A7FDEF7DB",
"type": "applied",
"action": "update",
"hash": "5a53abb526abd460a552135571f62b732ed34728",
"uid": "561d56bb176f7e000000000f",
"msg": "''"
----
=== delta_received/delta_record_received

There is one event emitted for each of the delta records returned from the cloud: 
* datasetId: the id of the dataset
* uid: the uid of the record
* code: the type of the event
* message: the operation to apply (e.g. create/update/delete)

=== sync_complete

This event is emitted when the sync loop is finished successfully:
* datasetId: the id of the dataset
* uid: the hash value of the dataset
* code: the type of the event
* message: the status (e.g. online, offline etc)

=== sync_failed

This event is emitted when there are errors during the sync loop

* datasetId: the id of the dataset
* uid: the hash value of the dataset
* code: the type of the event
* message: possible error messages (if available)

== Deprecated events:

The following events are deprecated and should not be used, they will be removed in a future release:

* delta_received with message "full dataset"

This is removed from some sdks (Android, and JS SDK), but still available in others (iOS). 
There is no individual record uid available in the notification message.
The Client App needs to call the list API to get the current available data.

== How to use events

To get the data in the Sync framework and save it using other ways, for example,  CoreData:

. Listen for the `local_update_applied`, `delta_received` and `delta_record_received` events. These events ensure the app is notified when there are changes made by either the local user or remote users.
The UID of the affected data record and the corresponding operation is available in the notifications. 
. Modify your app to read the data record using the given UID from the Sync framework first, and then modify the local data accordingly. 
. Notify users about failures/collisions
It is best to notify users about failures and collisions using the `remote_update_failed` and `collision_detected` events. 
The data could revert to the value that is valid in the cloud, but it might appear to the local user that the change was discarded without some sort of notification.
